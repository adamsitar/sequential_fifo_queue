#pragma once
#include <cassert>
#include <cstddef>
#include <cstdio>
#include <freelist.h>
#include <local_buffer.h>
#include <memory_resource>
#include <pointers/segmented_ptr.h>
#include <pointers/thin_ptr.h>
#include <result/result.h>
#include <types.h>

// Allocator whose job is to take in blocks from upstream, and dispense blocks
// of smaller size.
// The size of this type should be equal to the upstream block size.
// Metadata about allocations is stored within class as a member, therefore this
// class can only give out a fixed max amount of blocks.
template <size_t block_size_t, homogenous upstream_t, typename tag = void>
  requires is_power_of_two<block_size_t>
class unique_dynamic_buffer : public std::pmr::memory_resource {
  static constexpr size_t blocks_per_segment =
      upstream_t::block_size / block_size_t;

  static_assert(upstream_t::block_size >= block_size_t,
                "Upstream block size must be >= requested block size");
  static_assert(
      upstream_t::block_size % block_size_t == 0,
      "Upstream block size must be a multiple of requested block size");
  static_assert(blocks_per_segment > 0,
                "At least one block must fit in upstream block");

  using freelist_type = freelist_storage<block_size_t, blocks_per_segment>;
  using freelist_offset_type = typename freelist_type::offset_type;

private:
  struct segment_metadata {
    using block_type = typename freelist_type::block_type;
    upstream_t::pointer_type segment_ptr{nullptr};
    // Freelist metadata (external storage for freelist_storage)
    freelist_offset_type freelist_head{
        std::numeric_limits<freelist_offset_type>::max()};
    freelist_offset_type freelist_count{0};

    bool is_valid() const noexcept { return segment_ptr != nullptr; }
    bool is_empty() const noexcept { return freelist_count == 0; }
    bool is_full() const noexcept {
      return freelist_count >= blocks_per_segment;
    }

    bool owns_block(const block_type *block) const noexcept {
      if (!is_valid()) { return false; }

      auto *freelist = reinterpret_cast<const freelist_type *>(
          static_cast<void *>(segment_ptr));
      return freelist->owns(*block);
    }

    result<block_type *> try_allocate() noexcept {
      fail(!is_valid(), "invalid metadata").stacktrace();

      auto *freelist =
          reinterpret_cast<freelist_type *>(static_cast<void *>(segment_ptr));
      return ok(freelist->pop(freelist_head, freelist_count));
    }

    result<> deallocate(block_type *block, upstream_t *upstream) noexcept {
      auto *freelist =
          reinterpret_cast<freelist_type *>(static_cast<void *>(segment_ptr));
      ok(freelist->push(*block, freelist_head, freelist_count));

      if (is_empty()) {
        ok(upstream->deallocate_block(segment_ptr));
        segment_ptr = nullptr;
        return {};
      }
      return {};
    }
  };

  static constexpr size_t max_segment_count{upstream_t::block_size /
                                            sizeof(segment_metadata)};
  upstream_t *_upstream;
  // segment count, never decreased
  // one past the last valid index
  smallest_t<max_segment_count> _high_water_mark{0};
  // cache for last successful allocation segment
  smallest_t<max_segment_count> _alloc_cache{0};
  // cache for last successful pointer lookup
  mutable smallest_t<max_segment_count> _lookup_cache{0};
  std::array<segment_metadata, max_segment_count> _segments;

  using storage = segmented_ptr_storage<tag>;

public:
  // provides_management
  void reset() {};
  std::size_t size() const noexcept {};

  // provides_uniform_blocks
  static constexpr size_t block_size = block_size;
  static constexpr size_t block_align = block_size;
  static constexpr size_t max_block_count =
      blocks_per_segment * max_segment_count; // total max capacity
  static constexpr size_t total_size = block_size * max_block_count;
  using block_type = freelist_type::block_type;

  // provides_offset
  using unique_tag = tag;
  using offset_type = smallest_t<blocks_per_segment>; // within a segment

  // provides_pointer
  using pointer_type = basic_segmented_ptr<block_type, offset_type, unique_tag,
                                           smallest_t<max_segment_count>>;

  result<std::byte *> get_segment_base(uint8_t segment_id) const noexcept {
    fail(segment_id >= _high_water_mark, "invalid segment id");
    auto &metadata = _segments[segment_id];
    fail(!metadata.is_valid());

    return static_cast<std::byte *>(static_cast<void *>(metadata.segment_ptr));
  }

  // Find which segment owns a raw pointer
  // Called by segmented_ptr_storage during pointer conversion from void*
  result<size_t> find_segment_for_pointer(std::byte *ptr) const noexcept {
    auto *block = reinterpret_cast<block_type *>(ptr);

    // Try alloc cache first (spatial locality - recently allocated blocks)
    if (_alloc_cache < _high_water_mark) {
      if (_segments[_alloc_cache].is_valid() &&
          _segments[_alloc_cache].owns_block(block)) {
        _lookup_cache = _alloc_cache; // Warm lookup cache
        return _alloc_cache;
      }
    }

    // Try lookup cache (temporal locality - recently looked ok blocks)
    if (_lookup_cache < _high_water_mark && _lookup_cache != _alloc_cache) {
      if (_segments[_lookup_cache].is_valid() &&
          _segments[_lookup_cache].owns_block(block)) {
        return _lookup_cache;
      }
    }

    // Cache miss, perform linear scan
    for (size_t i = 0; i < _high_water_mark; ++i) {
      if (i == _lookup_cache || i == _alloc_cache) {
        continue; // Already tried
      }

      if (_segments[i].is_valid() && _segments[i].owns_block(block)) {
        _lookup_cache = i; // Update cache
        return i;
      }
    }

    return "not owned";
  }

  unique_dynamic_buffer(upstream_t *upstream) : _upstream(upstream) {
    unwrap(storage::register_buffer(this));
  }

  ~unique_dynamic_buffer() override {
    storage::unregister_buffer();

    for (auto &segment : std::span(_segments.data(), _high_water_mark)) {
      if (segment.is_valid()) {
        unwrap(_upstream->deallocate_block(segment.segment_ptr));
      }
    }
  }

  unique_dynamic_buffer(const unique_dynamic_buffer &) = delete;
  unique_dynamic_buffer &operator=(const unique_dynamic_buffer &) = delete;
  unique_dynamic_buffer(unique_dynamic_buffer &&) = delete;
  unique_dynamic_buffer &operator=(unique_dynamic_buffer &&) = delete;

  result<pointer_type> allocate_block() noexcept {
    if (_alloc_cache < _high_water_mark) {
      auto block_result = _segments[_alloc_cache].try_allocate();
      if (block_result) { return pointer_type(_alloc_cache, *block_result); }
    }

    for (auto i = 0; i < _high_water_mark; ++i) {
      if (i == _alloc_cache) { continue; }

      auto block_result = _segments[i].try_allocate();
      if (block_result) {
        _alloc_cache = i; // Update cache
        return pointer_type(i, *block_result);
      }
    }

    return allocate_new_segment();
  };

  result<> deallocate_block(pointer_type ptr) {
    fail(ptr == nullptr);

    size_t segment_id = ptr.get_segment_id();
    fail(segment_id >= _high_water_mark, "invalid segment");

    auto &metadata = _segments[segment_id];
    fail(!metadata.is_valid());

    auto *block = static_cast<block_type *>(static_cast<void *>(ptr));

    ok(metadata.deallocate(block, _upstream));
    return {};
  };

private:
  result<size_t> find_free_slot() const noexcept {
    for (size_t i = 0; i < max_segment_count; ++i) {
      if (!_segments[i].is_valid()) { return i; }
    }
    return "segment exhausted";
  }

  result<pointer_type> allocate_new_segment() noexcept {
    size_t slot = ok(find_free_slot());

    auto upstream_block = ok(_upstream->allocate_block());
    typename upstream_t::pointer_type upstream_ptr = upstream_block;

    void *placement_ptr = static_cast<void *>(upstream_ptr);

    // Construct freelist with metadata references - constructor automatically
    // initializes
    auto *freelist = new (placement_ptr) freelist_type(
        _segments[slot].freelist_head, _segments[slot].freelist_count);

    _segments[slot].segment_ptr = upstream_ptr;
    _alloc_cache = slot;

    if (slot >= _high_water_mark) { _high_water_mark = slot + 1; }

    return allocate_block();
  }

  void *do_allocate(size_t bytes, size_t alignment) override {
    if (bytes > block_size || alignment > block_size) { return nullptr; }

    return to_nullptr(allocate_block());
  }

  void do_deallocate(void *ptr, size_t bytes, size_t alignment) override {
    pointer_type seg_ptr{ptr};
    unwrap(deallocate_block(seg_ptr));
  }

  bool
  do_is_equal(const std::pmr::memory_resource &other) const noexcept override {
    return this == &other;
  }
};

#define dynamic_buffer(block_size, upstream_type)                              \
  unique_dynamic_buffer<block_size, upstream_type, decltype([] {})>

static_assert(homogenous<dynamic_buffer(256, local_buffer(2048, 16))>,
              "dynamic_buffer must implement homogeneous_allocator concept");
